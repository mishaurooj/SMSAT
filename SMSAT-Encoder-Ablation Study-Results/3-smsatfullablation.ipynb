{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T16:17:35.750100Z",
     "iopub.status.busy": "2025-08-11T16:17:35.749585Z",
     "iopub.status.idle": "2025-08-11T16:17:39.999795Z",
     "shell.execute_reply": "2025-08-11T16:17:39.999069Z",
     "shell.execute_reply.started": "2025-08-11T16:17:35.750069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting XlsxWriter\n",
      "  Downloading xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: XlsxWriter\n",
      "Successfully installed XlsxWriter-3.2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install XlsxWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T16:17:43.410592Z",
     "iopub.status.busy": "2025-08-11T16:17:43.410259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "Experiment 1/16: baseline_mse_all_aug_on\n",
      "====================================================================================================\n",
      "Discovered classes: ['Music', 'Normal(Silence)', 'SpiritualMeditation']\n",
      "Class file counts: {'Music': 47, 'Normal(Silence)': 48, 'SpiritualMeditation': 46} TOTAL: 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\muroo/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Self-Supervised Training on cuda\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "SMSAT/QMSAT Full Ablation Runner (robust, folder auto-discovery)\n",
    "\n",
    "- Auto-discovers class folders under root_dir (no hard-coded names).\n",
    "- Defensive augmentations (handles Windows/sox quirks; always crops to fixed length).\n",
    "- Clear sanity checks (prints per-class counts; fails early if dataset empty).\n",
    "- Same ablation matrix, metrics printing, linear eval, and Excel output.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import time\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "# -----------------------\n",
    "# 0. Reproducibility\n",
    "# -----------------------\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# -----------------------\n",
    "# 1. Augmentations\n",
    "# -----------------------\n",
    "\n",
    "@dataclass\n",
    "class AugConfig:\n",
    "    sample_rate: int = 16000\n",
    "    crop_size: int = 16000\n",
    "    use_noise: bool = True\n",
    "    noise_factor: float = 0.005\n",
    "    use_time_stretch: bool = True\n",
    "    time_stretch_range: Tuple[float, float] = (0.8, 1.2)\n",
    "    use_pitch_shift: bool = True\n",
    "    pitch_steps_range: Tuple[float, float] = (-2.0, 2.0)\n",
    "    use_spec_augment: bool = True  # applied on mel inside the model\n",
    "\n",
    "class AudioAugmentations:\n",
    "    def __init__(self, cfg: AugConfig):\n",
    "        self.cfg = cfg\n",
    "        self.sample_rate = cfg.sample_rate\n",
    "        self.crop_size = cfg.crop_size\n",
    "\n",
    "    def add_noise(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.cfg.use_noise:\n",
    "            return waveform\n",
    "        return waveform + torch.randn_like(waveform) * self.cfg.noise_factor\n",
    "\n",
    "    def time_stretch(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.cfg.use_time_stretch:\n",
    "            return waveform\n",
    "        rate = float(random.uniform(*self.cfg.time_stretch_range))\n",
    "        effects = [['speed', f'{rate}'], ['rate', f'{self.sample_rate}']]\n",
    "        try:\n",
    "            stretched, _ = torchaudio.sox_effects.apply_effects_tensor(\n",
    "                waveform.float().cpu(), self.sample_rate, effects\n",
    "            )\n",
    "            # Always crop/pad to keep shapes consistent\n",
    "            return self.random_crop(stretched)\n",
    "        except Exception:\n",
    "            return waveform\n",
    "\n",
    "    def pitch_shift_fn(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.cfg.use_pitch_shift:\n",
    "            return waveform\n",
    "        steps = float(random.uniform(*self.cfg.pitch_steps_range))\n",
    "        try:\n",
    "            # PitchShift prefers CPU float tensors\n",
    "            shift = torchaudio.transforms.PitchShift(self.sample_rate, n_steps=steps)\n",
    "            return shift(waveform.float().cpu())\n",
    "        except Exception:\n",
    "            return waveform\n",
    "\n",
    "    def random_crop(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        # waveform: (1, T) or (C, T). We keep channel count unchanged.\n",
    "        T = waveform.shape[-1]\n",
    "        if T > self.crop_size:\n",
    "            start = random.randint(0, T - self.crop_size)\n",
    "            return waveform[..., start:start + self.crop_size]\n",
    "        elif T < self.crop_size:\n",
    "            pad_amt = self.crop_size - T\n",
    "            return F.pad(waveform, (0, pad_amt))\n",
    "        return waveform\n",
    "\n",
    "    def __call__(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        # Defensive wrapper: never let aug errors kill the worker\n",
    "        x = waveform\n",
    "        try:\n",
    "            x = self.add_noise(x)\n",
    "            x = self.time_stretch(x)\n",
    "            x = self.pitch_shift_fn(x)\n",
    "        except Exception as e:\n",
    "            print(f\"[AUG WARN] {e}. Using original waveform.\")\n",
    "            x = waveform\n",
    "        # Final shape enforcement\n",
    "        x = self.random_crop(x)\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "# -----------------------\n",
    "# 2. Datasets (auto-discovery)\n",
    "# -----------------------\n",
    "\n",
    "# Will be filled dynamically from the folder names in root_dir\n",
    "LABELS: List[str] = []\n",
    "LABEL_TO_IDX: Dict[str, int] = {}\n",
    "\n",
    "def discover_labels(root_dir: str) -> List[str]:\n",
    "    classes = sorted([p.name for p in Path(root_dir).iterdir() if p.is_dir()])\n",
    "    if not classes:\n",
    "        raise RuntimeError(f\"No class subfolders found in '{root_dir}'.\")\n",
    "    return classes\n",
    "\n",
    "class QMSAT(Dataset):\n",
    "    \"\"\"Contrastive dataset: two augmented views + label string.\"\"\"\n",
    "    def __init__(self, root_dir, sample_rate=16000, transform: AudioAugmentations=None):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.transform = transform\n",
    "\n",
    "        # Discover classes dynamically\n",
    "        global LABELS, LABEL_TO_IDX\n",
    "        LABELS = discover_labels(root_dir)\n",
    "        LABEL_TO_IDX = {lab: i for i, lab in enumerate(LABELS)}\n",
    "\n",
    "        self.file_paths: List[str] = []\n",
    "        self.labels: List[str] = []\n",
    "\n",
    "        per_class_counts = {}\n",
    "        for sub_dir in LABELS:\n",
    "            folder = Path(root_dir) / sub_dir\n",
    "            wav_files = glob.glob(str(folder / '*.wav'))\n",
    "            self.file_paths.extend(wav_files)\n",
    "            self.labels.extend([sub_dir] * len(wav_files))\n",
    "            per_class_counts[sub_dir] = len(wav_files)\n",
    "\n",
    "        total = len(self.file_paths)\n",
    "        print(\"Discovered classes:\", LABELS)\n",
    "        print(\"Class file counts:\", per_class_counts, \"TOTAL:\", total)\n",
    "        if total == 0:\n",
    "            raise RuntimeError(f\"No .wav files found in {root_dir}/*/*.wav\")\n",
    "\n",
    "    def __len__(self): return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        if waveform.dim() == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            aug1 = self.transform(waveform.clone())\n",
    "            aug2 = self.transform(waveform.clone())\n",
    "        else:\n",
    "            # still enforce fixed length if no transform\n",
    "            aug1 = aug2 = AudioAugmentations(AugConfig(sample_rate=self.sample_rate)).random_crop(waveform)\n",
    "\n",
    "        # Final guards: mono and (1, T)\n",
    "        if aug1.dim() == 1: aug1 = aug1.unsqueeze(0)\n",
    "        if aug2.dim() == 1: aug2 = aug2.unsqueeze(0)\n",
    "        label = self.labels[idx]\n",
    "        return aug1, aug2, label\n",
    "\n",
    "class QMSAT_Eval(Dataset):\n",
    "    \"\"\"Supervised eval dataset: returns (waveform fixed len, label_idx) without aug.\"\"\"\n",
    "    def __init__(self, root_dir, sample_rate=16000, crop_size=16000):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "        # Use the same dynamic labels\n",
    "        global LABELS, LABEL_TO_IDX\n",
    "        if not LABELS:\n",
    "            LABELS = discover_labels(root_dir)\n",
    "            LABEL_TO_IDX = {lab: i for i, lab in enumerate(LABELS)}\n",
    "\n",
    "        self.paths: List[str] = []\n",
    "        self.labels: List[int] = []\n",
    "        per_class_counts = {}\n",
    "        for sub in LABELS:\n",
    "            folder = Path(root_dir) / sub\n",
    "            files = glob.glob(str(folder / '*.wav'))\n",
    "            self.paths.extend(files)\n",
    "            self.labels.extend([LABEL_TO_IDX[sub]] * len(files))\n",
    "            per_class_counts[sub] = len(files)\n",
    "\n",
    "        total = len(self.paths)\n",
    "        print(\"[Eval] Class file counts:\", per_class_counts, \"TOTAL:\", total)\n",
    "        if total == 0:\n",
    "            raise RuntimeError(f\"No .wav files found in {root_dir}/*/*.wav\")\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        wav, sr = torchaudio.load(p)\n",
    "        if wav.dim() == 1:\n",
    "            wav = wav.unsqueeze(0)\n",
    "        if sr != self.sample_rate:\n",
    "            wav = torchaudio.transforms.Resample(sr, self.sample_rate)(wav)\n",
    "        T = self.crop_size\n",
    "        if wav.shape[1] > T:\n",
    "            start = (wav.shape[1] - T) // 2\n",
    "            wav = wav[:, start:start + T]\n",
    "        elif wav.shape[1] < T:\n",
    "            wav = F.pad(wav, (0, T - wav.shape[1]))\n",
    "        return wav, self.labels[idx]\n",
    "\n",
    "# -----------------------\n",
    "# 3. Model\n",
    "# -----------------------\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    projection_dim: int = 128\n",
    "    pretrained: bool = True\n",
    "    freeze_backbone: bool = False\n",
    "    apply_spec_augment: bool = True  # apply SpecAug on mel inside the encoder\n",
    "\n",
    "class QSMATATSEncoder(nn.Module):\n",
    "    def __init__(self, cfg: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.mel_spec = MelSpectrogram(sample_rate=16000, n_mels=64)\n",
    "        self.db_transform = AmplitudeToDB()\n",
    "        self.freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=15) if cfg.apply_spec_augment else None\n",
    "\n",
    "        # torchvision v0.10.0 hub is fine for resnet18 weights\n",
    "        resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=cfg.pretrained)\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        if cfg.freeze_backbone:\n",
    "            for p in resnet.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-1])  # (B, 512, 1, 1)\n",
    "        self.projection = nn.Linear(512, cfg.projection_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,1,T) or (B,T)\n",
    "        if x.dim() == 3:\n",
    "            x = x.squeeze(1)\n",
    "        mel = self.mel_spec(x)          # (B, 64, time)\n",
    "        mel_db = self.db_transform(mel)  # (B, 64, time)\n",
    "        if self.freq_mask is not None:\n",
    "            mel_db = self.freq_mask(mel_db)\n",
    "        mel_db = mel_db.unsqueeze(1)     # (B,1,64,time)\n",
    "        feats = self.encoder(mel_db).squeeze(-1).squeeze(-1)  # (B,512)\n",
    "        proj = self.projection(feats)    # (B,D)\n",
    "        return proj\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "# -----------------------\n",
    "# 4. Train config / Loss\n",
    "# -----------------------\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    epochs: int = 30\n",
    "    batch_size: int = 128\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    loss_type: str = \"mse\"    # 'mse' or 'ntxent'\n",
    "    temperature: float = 0.1  # for NT-Xent\n",
    "    seed: int = 42\n",
    "\n",
    "def nt_xent_loss(z1: torch.Tensor, z2: torch.Tensor, temperature: float = 0.1) -> torch.Tensor:\n",
    "    B, _ = z1.shape\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "    z = torch.cat([z1, z2], dim=0)                   # (2B, D)\n",
    "    sim = torch.matmul(z, z.t()) / temperature       # (2B, 2B)\n",
    "    mask = torch.eye(2 * B, device=z.device, dtype=torch.bool)\n",
    "    sim.masked_fill_(mask, -9e15)\n",
    "    pos = torch.cat([torch.diag(sim, B), torch.diag(sim, -B)], dim=0)  # (2B,)\n",
    "    denom = torch.logsumexp(sim, dim=1)\n",
    "    loss = -pos + denom\n",
    "    return loss.mean()\n",
    "\n",
    "# -----------------------\n",
    "# 5. SSL training (one experiment)\n",
    "# -----------------------\n",
    "\n",
    "def train_ssl_one_experiment(\n",
    "    root_dir: str,\n",
    "    aug_cfg: AugConfig,\n",
    "    model_cfg: ModelConfig,\n",
    "    train_cfg: TrainConfig,\n",
    "    num_workers: int = 0,     # start at 0 on Windows to get real errors; raise to 2 later\n",
    "    return_model: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    set_seed(train_cfg.seed)\n",
    "\n",
    "    augmentations = AudioAugmentations(aug_cfg)\n",
    "    dataset = QMSAT(root_dir=root_dir, sample_rate=aug_cfg.sample_rate, transform=augmentations)\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        raise RuntimeError(f\"Dataset is empty. Check path: {root_dir}\")\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    if train_size == 0 or val_size == 0:\n",
    "        raise RuntimeError(f\"Split too small (train={train_size}, val={val_size}). Add more data.\")\n",
    "\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size],\n",
    "                                              generator=torch.Generator().manual_seed(train_cfg.seed))\n",
    "\n",
    "    pin = (device.type == \"cuda\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_cfg.batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=pin, persistent_workers=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=train_cfg.batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=pin, persistent_workers=False)\n",
    "\n",
    "    # Pass SpecAug flag from aug_cfg into model_cfg.apply_spec_augment\n",
    "    model_cfg = ModelConfig(**{**asdict(model_cfg), \"apply_spec_augment\": aug_cfg.use_spec_augment})\n",
    "    model = QSMATATSEncoder(model_cfg).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=train_cfg.lr, weight_decay=train_cfg.weight_decay)\n",
    "\n",
    "    train_losses, train_cosine = [], []\n",
    "    val_losses, val_cosine = [], []\n",
    "    epoch_times = []\n",
    "\n",
    "    best_val = float('inf')\n",
    "    best_epoch = -1\n",
    "    best_snapshot: Dict[str, float] = {}\n",
    "\n",
    "    total_start = time.time()\n",
    "    print(f\"Starting Self-Supervised Training on {device}\")\n",
    "    for epoch in range(1, train_cfg.epochs + 1):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        run_loss, run_cos, n_batches = 0.0, 0.0, 0\n",
    "\n",
    "        for a1, a2, _ in train_loader:\n",
    "            a1, a2 = a1.to(device), a2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            p1 = model(a1)\n",
    "            p2 = model(a2)\n",
    "\n",
    "            if train_cfg.loss_type == 'mse':\n",
    "                loss = F.mse_loss(p1, p2)\n",
    "            elif train_cfg.loss_type == 'ntxent':\n",
    "                loss = nt_xent_loss(p1, p2, temperature=train_cfg.temperature)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown loss_type={train_cfg.loss_type}\")\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            run_loss += loss.item()\n",
    "            cos = F.cosine_similarity(p1, p2, dim=1).mean().item()\n",
    "            run_cos += cos\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_train_loss = run_loss / max(n_batches, 1)\n",
    "        avg_train_cos = run_cos / max(n_batches, 1)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        vloss, vcos, v_batches = 0.0, 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for a1, a2, _ in val_loader:\n",
    "                a1, a2 = a1.to(device), a2.to(device)\n",
    "                p1 = model.encode(a1)\n",
    "                p2 = model.encode(a2)\n",
    "\n",
    "                if train_cfg.loss_type == 'mse':\n",
    "                    l = F.mse_loss(p1, p2)\n",
    "                else:\n",
    "                    l = nt_xent_loss(p1, p2, temperature=train_cfg.temperature)\n",
    "\n",
    "                vloss += l.item()\n",
    "                vcos += F.cosine_similarity(p1, p2, dim=1).mean().item()\n",
    "                v_batches += 1\n",
    "\n",
    "        avg_val_loss = vloss / max(v_batches, 1)\n",
    "        avg_val_cos  = vcos  / max(v_batches, 1)\n",
    "\n",
    "        epoch_time = time.time() - t0\n",
    "        epoch_times.append(epoch_time)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_cosine.append(avg_train_cos)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_cosine.append(avg_val_cos)\n",
    "\n",
    "        print(f\"[Epoch {epoch:02d}] Train Loss: {avg_train_loss:.4f} | Train Cos: {avg_train_cos:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | Val Cos: {avg_val_cos:.4f} | Time: {epoch_time:.2f} sec\")\n",
    "\n",
    "        if avg_val_loss < best_val:\n",
    "            best_val = avg_val_loss\n",
    "            best_epoch = epoch\n",
    "            best_snapshot = {\n",
    "                \"best_epoch\": best_epoch,\n",
    "                \"best_val_loss\": float(avg_val_loss),\n",
    "                \"best_val_cosine\": float(avg_val_cos),\n",
    "                \"train_loss_at_best\": float(avg_train_loss),\n",
    "                \"train_cos_at_best\": float(avg_train_cos),\n",
    "            }\n",
    "\n",
    "    total_time_sec = time.time() - total_start\n",
    "    print(f\"Total Training Time: {total_time_sec/60:.2f} minutes\")\n",
    "\n",
    "    history = {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_cosine\": train_cosine,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_cosine\": val_cosine,\n",
    "        \"epoch_times\": epoch_times,\n",
    "        \"total_training_time_sec\": total_time_sec,\n",
    "        \"best_snapshot\": best_snapshot,\n",
    "    }\n",
    "    if return_model:\n",
    "        history[\"model\"] = model\n",
    "    else:\n",
    "        history[\"model_state_dict\"] = model.state_dict()\n",
    "    return history\n",
    "\n",
    "# -----------------------\n",
    "# 6. Linear evaluation\n",
    "# -----------------------\n",
    "\n",
    "def linear_eval_train_val_accuracy(\n",
    "    model: nn.Module,\n",
    "    root_dir: str,\n",
    "    batch_size: int = 256,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 1e-3,\n",
    "    seed: int = 42,\n",
    "    crop_size: int = 16000\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Freeze encoder. Train a single linear layer on train split embeddings.\n",
    "    Return (train_acc, val_acc).\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    full = QMSAT_Eval(root_dir=root_dir, sample_rate=16000, crop_size=crop_size)\n",
    "    n = len(full); n_train = int(0.8*n); n_val = n - n_train\n",
    "    if n_train == 0 or n_val == 0:\n",
    "        raise RuntimeError(f\"Not enough data for linear eval split (n={n}).\")\n",
    "    train_set, val_set = random_split(full, [n_train, n_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    pin = (device.type == \"cuda\")\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=pin)\n",
    "    val_loader   = DataLoader(val_set,   batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=pin)\n",
    "\n",
    "    model.eval()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # embedding dimension\n",
    "    with torch.no_grad():\n",
    "        x0, _ = next(iter(train_loader))\n",
    "        emb0 = model.encode(x0.to(device))\n",
    "        dim = emb0.shape[1]\n",
    "\n",
    "    clf = nn.Linear(dim, len(LABELS)).to(device)\n",
    "    opt = torch.optim.Adam(clf.parameters(), lr=lr)\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def get_embeddings(loader):\n",
    "        embs, ys = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x = x.to(device)\n",
    "                z = model.encode(x)\n",
    "                embs.append(z.cpu())\n",
    "                ys.append(torch.tensor(y))\n",
    "        return torch.cat(embs, 0), torch.cat(ys, 0)\n",
    "\n",
    "    Ztr, Ytr = get_embeddings(train_loader)\n",
    "    Zva, Yva = get_embeddings(val_loader)\n",
    "    Ztr = Ztr.to(device); Ytr = Ytr.to(device)\n",
    "    Zva = Zva.to(device); Yva = Yva.to(device)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        clf.train()\n",
    "        opt.zero_grad()\n",
    "        logits = clf(Ztr)\n",
    "        loss = ce(logits, Ytr)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    clf.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_tr = clf(Ztr).argmax(1)\n",
    "        pred_va = clf(Zva).argmax(1)\n",
    "        train_acc = (pred_tr == Ytr).float().mean().item()\n",
    "        val_acc   = (pred_va == Yva).float().mean().item()\n",
    "\n",
    "    return float(train_acc), float(val_acc)\n",
    "\n",
    "# -----------------------\n",
    "# 7. Excel writer\n",
    "# -----------------------\n",
    "\n",
    "def write_results_to_excel(\n",
    "    excel_path: str,\n",
    "    run_summaries: List[Dict[str, Any]],\n",
    "    per_epoch_rows: List[Dict[str, Any]]\n",
    "):\n",
    "    os.makedirs(os.path.dirname(excel_path), exist_ok=True)\n",
    "    df_runs = pd.DataFrame(run_summaries)\n",
    "    df_epochs = pd.DataFrame(per_epoch_rows)\n",
    "    with pd.ExcelWriter(excel_path, engine=\"xlsxwriter\") as writer:\n",
    "        df_runs.to_excel(writer, sheet_name=\"runs\", index=False)\n",
    "        df_epochs.to_excel(writer, sheet_name=\"per_epoch\", index=False)\n",
    "    print(f\"Saved Excel to: {excel_path}\")\n",
    "\n",
    "# -----------------------\n",
    "# 8. Experiments list (ALL ablations)\n",
    "# -----------------------\n",
    "\n",
    "@dataclass\n",
    "class Experiment:\n",
    "    name: str\n",
    "    aug: AugConfig\n",
    "    model: ModelConfig\n",
    "    train: TrainConfig\n",
    "    notes: str = \"\"\n",
    "    do_linear_eval: bool = True  # compute accuracies for all runs\n",
    "\n",
    "def build_ablation_list(baseline: Experiment) -> List[Experiment]:\n",
    "    base = baseline\n",
    "\n",
    "    exps: List[Experiment] = [\n",
    "        base,  # Baseline: all augmentations on + MSE\n",
    "\n",
    "        # ---- Augmentation ablations\n",
    "        Experiment(\n",
    "            name=\"no_spec_augment\",\n",
    "            aug=AugConfig(**{**asdict(base.aug), \"use_spec_augment\": False}),\n",
    "            model=base.model, train=base.train,\n",
    "            notes=\"SpecAug (mel masking) OFF\"\n",
    "        ),\n",
    "        Experiment(\n",
    "            name=\"no_pitch_shift\",\n",
    "            aug=AugConfig(**{**asdict(base.aug), \"use_pitch_shift\": False}),\n",
    "            model=base.model, train=base.train,\n",
    "            notes=\"Pitch shift OFF\"\n",
    "        ),\n",
    "        Experiment(\n",
    "            name=\"no_time_stretch\",\n",
    "            aug=AugConfig(**{**asdict(base.aug), \"use_time_stretch\": False}),\n",
    "            model=base.model, train=base.train,\n",
    "            notes=\"Time stretch OFF\"\n",
    "        ),\n",
    "        Experiment(\n",
    "            name=\"no_noise\",\n",
    "            aug=AugConfig(**{**asdict(base.aug), \"use_noise\": False}),\n",
    "            model=base.model, train=base.train,\n",
    "            notes=\"Additive noise OFF\"\n",
    "        ),\n",
    "        Experiment(\n",
    "            name=\"shorter_crop_0p5s\",\n",
    "            aug=AugConfig(**{**asdict(base.aug), \"crop_size\": int(0.5 * base.aug.sample_rate)}),\n",
    "            model=base.model, train=base.train,\n",
    "            notes=\"Shorter crops (0.5s)\"\n",
    "        ),\n",
    "\n",
    "        # ---- Model ablations\n",
    "        Experiment(\n",
    "            name=\"proj64\",\n",
    "            aug=base.aug,\n",
    "            model=ModelConfig(**{**asdict(base.model), \"projection_dim\": 64}),\n",
    "            train=base.train,\n",
    "            notes=\"Projection 64\"\n",
    "        ),\n",
    "        Experiment(\n",
    "            name=\"proj256\",\n",
    "            aug=base.aug,\n",
    "            model=ModelConfig(**{**asdict(base.model), \"projection_dim\": 256}),\n",
    "            train=base.train,\n",
    "            notes=\"Projection 256\"\n",
    "        ),\n",
    "        Experiment(\n",
    "            name=\"no_pretrain\",\n",
    "            aug=base.aug,\n",
    "            model=ModelConfig(**{**asdict(base.model), \"pretrained\": False}),\n",
    "            train=base.train,\n",
    "            notes=\"ResNet18 from scratch\"\n",
    "        ),\n",
    "        Experiment(\n",
    "            name=\"freeze_backbone\",\n",
    "            aug=base.aug,\n",
    "            model=ModelConfig(**{**asdict(base.model), \"freeze_backbone\": True}),\n",
    "            train=base.train,\n",
    "            notes=\"Freeze backbone\"\n",
    "        ),\n",
    "\n",
    "        # ---- Loss ablations\n",
    "        Experiment(\n",
    "            name=\"ntxent_temp_0p1\",\n",
    "            aug=base.aug,\n",
    "            model=base.model,\n",
    "            train=TrainConfig(**{**asdict(base.train), \"loss_type\": \"ntxent\", \"temperature\": 0.1}),\n",
    "            notes=\"NT-Xent (τ=0.1)\"\n",
    "        ),\n",
    "        Experiment(\n",
    "            name=\"ntxent_temp_0p5\",\n",
    "            aug=base.aug,\n",
    "            model=base.model,\n",
    "            train=TrainConfig(**{**asdict(base.train), \"loss_type\": \"ntxent\", \"temperature\": 0.5}),\n",
    "            notes=\"NT-Xent (τ=0.5)\"\n",
    "        ),\n",
    "\n",
    "        # ---- Optim ablations\n",
    "        Experiment(\n",
    "            name=\"lr_3e-4\",\n",
    "            aug=base.aug,\n",
    "            model=base.model,\n",
    "            train=TrainConfig(**{**asdict(base.train), \"lr\": 3e-4}),\n",
    "            notes=\"Lower LR 3e-4\"\n",
    "        ),\n",
    "        Experiment(\n",
    "            name=\"lr_1e-4\",\n",
    "            aug=base.aug,\n",
    "            model=base.model,\n",
    "            train=TrainConfig(**{**asdict(base.train), \"lr\": 1e-4}),\n",
    "            notes=\"Lower LR 1e-4\"\n",
    "        ),\n",
    "        Experiment(\n",
    "            name=\"batch_64\",\n",
    "            aug=base.aug,\n",
    "            model=base.model,\n",
    "            train=TrainConfig(**{**asdict(base.train), \"batch_size\": 64}),\n",
    "            notes=\"Smaller batch size 64\"\n",
    "        ),\n",
    "\n",
    "        # ---- Special: original recipe style\n",
    "        Experiment(\n",
    "            name=\"original_99p75_ablation\",\n",
    "            aug=base.aug,\n",
    "            model=ModelConfig(**{**asdict(base.model), \"projection_dim\": 256}),\n",
    "            train=TrainConfig(**{**asdict(base.train), \"epochs\": 40, \"loss_type\": \"ntxent\", \"temperature\": 0.1}),\n",
    "            notes=\"NT-Xent τ=0.1, proj=256, 40 epochs\"\n",
    "        ),\n",
    "    ]\n",
    "    return exps\n",
    "\n",
    "# -----------------------\n",
    "# 9. Run all ablations\n",
    "# -----------------------\n",
    "\n",
    "def run_all_ablations():\n",
    "    # TODO: adjust these two paths for your machine\n",
    "    root_dir = r\"C:\\Users\\muroo\\Downloads\\archive\\ATS-data\"   # <--- your dataset root (matches your screenshot)\n",
    "    out_dir  = r\"C:\\Users\\muroo\\Downloads\\archive\"            # <--- where to save Excel & checkpoints\n",
    "\n",
    "    excel_path = os.path.join(out_dir, 'smsat_ablation_results.xlsx')\n",
    "\n",
    "    baseline = Experiment(\n",
    "        name=\"baseline_mse_all_aug_on\",\n",
    "        aug=AugConfig(sample_rate=16000, crop_size=16000,\n",
    "                      use_noise=True, use_time_stretch=True,\n",
    "                      use_pitch_shift=True, use_spec_augment=True),\n",
    "        model=ModelConfig(projection_dim=128, pretrained=True, freeze_backbone=False, apply_spec_augment=True),\n",
    "        train=TrainConfig(epochs=30, batch_size=128, lr=1e-3, weight_decay=1e-4, loss_type='mse', seed=42),\n",
    "        notes=\"All augmentations ON (noise, time-stretch, pitch-shift, SpecAug in mel); MSE objective.\",\n",
    "        do_linear_eval=True\n",
    "    )\n",
    "\n",
    "    experiments = build_ablation_list(baseline)\n",
    "\n",
    "    run_summaries: List[Dict[str, Any]] = []\n",
    "    per_epoch_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for i, exp in enumerate(experiments, start=1):\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(f\"Experiment {i}/{len(experiments)}: {exp.name}\")\n",
    "        print(\"=\"*100)\n",
    "\n",
    "        # ----- SSL training -----\n",
    "        hist = train_ssl_one_experiment(\n",
    "            root_dir=root_dir,\n",
    "            aug_cfg=exp.aug,\n",
    "            model_cfg=exp.model,\n",
    "            train_cfg=exp.train,\n",
    "            num_workers=0,   # start with 0 to surface issues; switch to 2 after it's stable\n",
    "            return_model=True\n",
    "        )\n",
    "\n",
    "        # Per-epoch rows for Excel\n",
    "        for ep, (trL, trC, vL, vC, tS) in enumerate(zip(\n",
    "            hist[\"train_losses\"], hist[\"train_cosine\"],\n",
    "            hist[\"val_losses\"], hist[\"val_cosine\"],\n",
    "            hist[\"epoch_times\"]\n",
    "        ), start=1):\n",
    "            per_epoch_rows.append({\n",
    "                \"exp_id\": i,\n",
    "                \"exp_name\": exp.name,\n",
    "                \"epoch\": ep,\n",
    "                \"train_loss\": trL,\n",
    "                \"train_cosine\": trC,\n",
    "                \"val_loss\": vL,\n",
    "                \"val_cosine\": vC,\n",
    "                \"epoch_time_sec\": tS\n",
    "            })\n",
    "\n",
    "        best = hist[\"best_snapshot\"]\n",
    "        total_sec = hist[\"total_training_time_sec\"]\n",
    "        total_minutes = total_sec / 60.0\n",
    "\n",
    "        # Save model checkpoint\n",
    "        ckpt_path = os.path.join(out_dir, f\"{exp.name}_encoder.pth\")\n",
    "        torch.save(hist[\"model\"].state_dict(), ckpt_path)\n",
    "        print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "        # ----- Linear evaluation (train & val accuracy) -----\n",
    "        train_acc, val_acc = (None, None)\n",
    "        if exp.do_linear_eval:\n",
    "            print(\"Running linear evaluation (frozen encoder) to compute accuracy...\")\n",
    "            train_acc, val_acc = linear_eval_train_val_accuracy(\n",
    "                model=hist[\"model\"].to(device),\n",
    "                root_dir=root_dir,\n",
    "                batch_size=256,\n",
    "                epochs=10,\n",
    "                lr=1e-3,\n",
    "                seed=exp.train.seed,\n",
    "                crop_size=exp.aug.crop_size\n",
    "            )\n",
    "            print(f\"[Linear Eval] Train Accuracy: {train_acc*100:.2f}% | Val Accuracy: {val_acc*100:.2f}%\")\n",
    "\n",
    "        # ----- Run-level summary -----\n",
    "        summary = {\n",
    "            \"exp_id\": i,\n",
    "            \"exp_name\": exp.name,\n",
    "            \"notes\": exp.notes,\n",
    "            # Aug toggles\n",
    "            \"use_noise\": exp.aug.use_noise,\n",
    "            \"use_time_stretch\": exp.aug.use_time_stretch,\n",
    "            \"use_pitch_shift\": exp.aug.use_pitch_shift,\n",
    "            \"use_spec_augment\": exp.aug.use_spec_augment,\n",
    "            \"crop_size\": exp.aug.crop_size,\n",
    "            # Model\n",
    "            \"projection_dim\": exp.model.projection_dim,\n",
    "            \"pretrained\": exp.model.pretrained,\n",
    "            \"freeze_backbone\": exp.model.freeze_backbone,\n",
    "            # Train\n",
    "            \"loss_type\": exp.train.loss_type,\n",
    "            \"temperature\": exp.train.temperature if exp.train.loss_type == \"ntxent\" else None,\n",
    "            \"epochs\": exp.train.epochs,\n",
    "            \"batch_size\": exp.train.batch_size,\n",
    "            \"lr\": exp.train.lr,\n",
    "            \"weight_decay\": exp.train.weight_decay,\n",
    "            \"seed\": exp.train.seed,\n",
    "            # Best snapshot\n",
    "            \"best_epoch\": best.get(\"best_epoch\", None),\n",
    "            \"best_val_loss\": best.get(\"best_val_loss\", None),\n",
    "            \"best_val_cosine\": best.get(\"best_val_cosine\", None),\n",
    "            \"train_loss_at_best\": best.get(\"train_loss_at_best\", None),\n",
    "            \"train_cos_at_best\": best.get(\"train_cos_at_best\", None),\n",
    "            # Timing\n",
    "            \"total_time_sec\": total_sec,\n",
    "            \"training_time_minutes\": total_minutes,\n",
    "            # Linear eval accuracies\n",
    "            \"linear_train_acc\": train_acc,\n",
    "            \"linear_val_acc\": val_acc\n",
    "        }\n",
    "        run_summaries.append(summary)\n",
    "\n",
    "    # ----- Write Excel -----\n",
    "    write_results_to_excel(excel_path, run_summaries, per_epoch_rows)\n",
    "    print(\"\\nAll experiments finished.\")\n",
    "    print(f\"Excel summary: {excel_path}\")\n",
    "    print(\"Sheets:\")\n",
    "    print(\" - runs: per-experiment summary (includes linear_train_acc, linear_val_acc, training_time_minutes)\")\n",
    "    print(\" - per_epoch: full SSL learning curves for every run\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_all_ablations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python jethexa",
   "language": "python",
   "name": "jethexa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

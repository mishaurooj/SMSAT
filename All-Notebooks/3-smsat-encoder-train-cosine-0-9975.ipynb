{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11180977,"sourceType":"datasetVersion","datasetId":6978917}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install XlsxWriter","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T07:04:59.981471Z","iopub.execute_input":"2025-03-27T07:04:59.981838Z","iopub.status.idle":"2025-03-27T07:05:04.633021Z","shell.execute_reply.started":"2025-03-27T07:04:59.981812Z","shell.execute_reply":"2025-03-27T07:05:04.632053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os \nimport random\nimport glob\nimport numpy as np\nimport time\nfrom pathlib import Path\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchaudio\nfrom torchaudio.transforms import MelSpectrogram, AmplitudeToDB\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nimport pandas as pd\n\n# ====================\n# 1. Data Augmentations\n# ====================\n\nclass AudioAugmentations:\n    def __init__(self, sample_rate=16000, noise_factor=0.005, time_stretch_range=(0.8, 1.2), crop_size=16000):\n        self.sample_rate = sample_rate\n        self.noise_factor = noise_factor\n        self.time_stretch_range = time_stretch_range\n        self.crop_size = crop_size\n        self.pitch_shift = torchaudio.transforms.PitchShift(sample_rate, n_steps=random.uniform(-2, 2))\n        self.spec_augment = torchaudio.transforms.FrequencyMasking(freq_mask_param=15)\n    \n    def add_noise(self, waveform):\n        noise = torch.randn_like(waveform) * self.noise_factor\n        return waveform + noise\n\n    def time_stretch(self, waveform):\n        rate = random.uniform(*self.time_stretch_range)\n        effects = [['speed', f'{rate}'], ['rate', f'{self.sample_rate}']]\n        try:\n            stretched, _ = torchaudio.sox_effects.apply_effects_tensor(waveform, self.sample_rate, effects)\n            return stretched\n        except OSError:\n            return waveform\n\n    def pitch_shift_fn(self, waveform):\n        return self.pitch_shift(waveform)\n\n    def apply_spec_augment(self, waveform):\n        return self.spec_augment(waveform)\n\n    def random_crop(self, waveform):\n        if waveform.shape[1] > self.crop_size:\n            max_start = waveform.shape[1] - self.crop_size\n            start = random.randint(0, max_start)\n            return waveform[:, start:start+self.crop_size]\n        else:\n            pad_amt = self.crop_size - waveform.shape[1]\n            return F.pad(waveform, (0, pad_amt))\n\n    def __call__(self, waveform):\n        if random.random() < 0.5:\n            waveform = self.add_noise(waveform)\n        if random.random() < 0.5:\n            waveform = self.time_stretch(waveform)\n        if random.random() < 0.5:\n            waveform = self.pitch_shift_fn(waveform)\n        waveform = self.random_crop(waveform)\n        if random.random() < 0.5:\n            waveform = self.apply_spec_augment(waveform)\n        return waveform\n\n# ====================\n# 2. Contrastive Audio Dataset\n# ====================\n\nclass QMSAT(Dataset):  # Renamed dataset to QMSAT\n    def __init__(self, root_dir, sample_rate=16000, transform=None):\n        self.sample_rate = sample_rate\n        self.file_paths = []\n        self.labels = []\n        self.transform = transform\n        # Assumes dataset subdirectories: 'Quran-tilawat', 'Calm-Music', 'Normal-surrounding-voices'\n        for sub_dir in ['Tilawat-e-QuranPak', 'Music', 'Normal(Silence)']:\n            folder = Path(root_dir) / sub_dir\n            wav_files = glob.glob(str(folder / '*.wav'))\n            self.file_paths.extend(wav_files)\n            self.labels.extend([sub_dir] * len(wav_files))\n    \n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        file_path = self.file_paths[idx]\n        waveform, sr = torchaudio.load(file_path)\n        if sr != self.sample_rate:\n            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n        if self.transform is not None:\n            # Detach augmented outputs so gradients don't flow from DataLoader collation.\n            aug1 = self.transform(waveform.clone()).detach()\n            aug2 = self.transform(waveform.clone()).detach()\n        else:\n            aug1, aug2 = waveform, waveform\n        label = self.labels[idx]\n        return aug1, aug2, label\n\n# ====================\n# 3. QSMAT ATS Encoder\n# ====================\n\nclass QSMATATSEncoder(nn.Module):  # Renamed encoder to QSMAT ATS Encoder\n    def __init__(self, projection_dim=128):\n        super(QSMATATSEncoder, self).__init__()\n        self.mel_spec = MelSpectrogram(sample_rate=16000, n_mels=64)\n        self.db_transform = AmplitudeToDB()\n        # Load a pretrained ResNet18 (from torch.hub here)\n        resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n        # Modify first conv layer for single-channel input.\n        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.encoder = nn.Sequential(*list(resnet.children())[:-1])\n        self.projection = nn.Linear(512, projection_dim)\n    \n    def forward(self, x):\n        if x.dim() == 3:\n            x = x.squeeze(1)\n        mel = self.mel_spec(x)\n        mel_db = self.db_transform(mel).unsqueeze(1)\n        features = self.encoder(mel_db).squeeze(-1).squeeze(-1)\n        projection = self.projection(features)\n        return projection\n    \n    def encode(self, x):\n        return self.forward(x)\n\n# ====================\n# 4. Training and Validation Functions for Self-Supervised Learning\n# ====================\n\ndef train_self_supervised(model, train_loader, val_loader, optimizer, device, epochs=30):\n    model.train()\n    train_losses = []\n    train_cosine = []  # average cosine similarity on training batches\n    val_losses = []\n    val_cosine = []    # average cosine similarity on validation batches\n    epoch_times = []\n    total_start_time = time.time()\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        running_cosine = 0.0\n        batch_count = 0\n        epoch_start = time.time()\n        for aug1, aug2, _ in train_loader:\n            aug1, aug2 = aug1.to(device), aug2.to(device)\n            optimizer.zero_grad()\n            proj1 = model(aug1)\n            proj2 = model(aug2)\n            loss = F.mse_loss(proj1, proj2)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            # Compute cosine similarity as additional metric.\n            cos_sim = F.cosine_similarity(proj1, proj2, dim=1).mean().item()\n            running_cosine += cos_sim\n            batch_count += 1\n        avg_train_loss = running_loss / batch_count\n        avg_train_cosine = running_cosine / batch_count\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        val_running_cosine = 0.0\n        val_batch_count = 0\n        with torch.no_grad():\n            for aug1, aug2, _ in val_loader:\n                aug1, aug2 = aug1.to(device), aug2.to(device)\n                proj1 = model.encode(aug1)\n                proj2 = model.encode(aug2)\n                loss = F.mse_loss(proj1, proj2)\n                val_running_loss += loss.item()\n                cos_sim = F.cosine_similarity(proj1, proj2, dim=1).mean().item()\n                val_running_cosine += cos_sim\n                val_batch_count += 1\n        avg_val_loss = val_running_loss / val_batch_count\n        avg_val_cosine = val_running_cosine / val_batch_count\n\n        epoch_time = time.time() - epoch_start\n        epoch_times.append(epoch_time)\n        train_losses.append(avg_train_loss)\n        train_cosine.append(avg_train_cosine)\n        val_losses.append(avg_val_loss)\n        val_cosine.append(avg_val_cosine)\n        \n        print(f\"[Epoch {epoch+1:02d}] Train Loss: {avg_train_loss:.4f} | Train Cosine: {avg_train_cosine:.4f} | \"\n              f\"Val Loss: {avg_val_loss:.4f} | Val Cosine: {avg_val_cosine:.4f} | Time: {epoch_time:.2f} sec\")\n    \n    total_training_time = time.time() - total_start_time\n    print(f\"Total Training Time: {total_training_time/60:.2f} minutes\")\n    return train_losses, train_cosine, val_losses, val_cosine, epoch_times, total_training_time\n\n\ndef plot_loss_curves(train_losses, val_losses):\n    epochs = range(1, len(train_losses) + 1)\n    plt.figure(figsize=(10,6))\n    plt.plot(epochs, train_losses, 'o-', label='Train Loss', color='blue')\n    plt.plot(epochs, val_losses, 's-', label='Validation Loss', color='red')\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"MSE Loss\")\n    plt.title(\"Self-Supervised Training Loss Curves\")\n    plt.grid(True)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\ndef plot_cosine_curves(train_cosine, val_cosine):\n    epochs = range(1, len(train_cosine) + 1)\n    plt.figure(figsize=(10,6))\n    plt.plot(epochs, train_cosine, 'o-', label='Train Cosine Similarity', color='blue')\n    plt.plot(epochs, val_cosine, 's-', label='Validation Cosine Similarity', color='red')\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cosine Similarity\")\n    plt.title(\"Cosine Similarity Curves\")\n    plt.grid(True)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n# ====================\n# 5. Embeddings & Statistical Analysis Functions\n# ====================\n\ndef extract_audio_labels(batch):\n    # For analysis, use the first augmented view.\n    aug1, aug2, labels = batch\n    return aug1, labels\n\ndef compute_and_plot_statistics(model, dataloader, device, method='tsne'):\n    \"\"\"\n    Computes class centroids from reduced embeddings, calculates inter-class distances,\n    and plots a bar graph (for inter-class distances) along with a boxplot of intra-class\n    distance distributions.\n    \"\"\"\n    model.eval()\n    all_embeddings = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in dataloader:\n            audio, labels = extract_audio_labels(batch)\n            audio = audio.to(device)\n            embeddings = model.encode(audio)\n            all_embeddings.append(embeddings.cpu().numpy())\n            all_labels.extend(labels)\n    all_embeddings = np.concatenate(all_embeddings, axis=0)\n    \n    # Dimensionality reduction\n    if method == 'tsne':\n        reducer = TSNE(n_components=2, perplexity=30, random_state=42)\n    else:\n        reducer = PCA(n_components=2)\n    reduced_embeddings = reducer.fit_transform(all_embeddings)\n    \n    unique_labels = ['Tilawat-e-QuranPak', 'Music', 'Normal(Silence)']\n    \n    # Compute centroids and intra-class distances.\n    centroids = {}\n    intra_class_dists = {}\n    for label in unique_labels:\n        idx = [i for i, lab in enumerate(all_labels) if lab == label]\n        emb_class = reduced_embeddings[idx]\n        centroid = emb_class.mean(axis=0)\n        centroids[label] = centroid\n        dists = np.linalg.norm(emb_class - centroid, axis=1)\n        intra_class_dists[label] = dists\n    \n    # Compute inter-class distances from Quran centroid.\n    dist_quran_calm = np.linalg.norm(centroids[\"Tilawat-e-QuranPak\"] - centroids[\"Music\"])\n    dist_quran_noise = np.linalg.norm(centroids[\"Tilawat-e-QuranPak\"] - centroids[\"Normal(Silence)\"])\n    \n    print(\"Inter-class Distances:\")\n    print(f\"Distance between Quran-tilawat and Calm-Music: {dist_quran_calm:.2f}\")\n    print(f\"Distance between Quran-tilawat and Normal-surrounding-voices: {dist_quran_noise:.2f}\")\n    \n    for label in unique_labels:\n        mean_dist = np.mean(intra_class_dists[label])\n        std_dist = np.std(intra_class_dists[label])\n        print(f\"{label} - Mean intra-class distance: {mean_dist:.2f} (Std: {std_dist:.2f})\")\n    \n    # Bar graph for inter-class distances.\n    plt.figure(figsize=(6,4))\n    bars = [\"Quran vs Calm\", \"Quran vs Noise\"]\n    distances_bar = [dist_quran_calm, dist_quran_noise]\n    plt.bar(bars, distances_bar, color=['red', 'red'])\n    plt.ylabel(\"Euclidean Distance\")\n    plt.title(\"Inter-Class Distances\")\n    for i, v in enumerate(distances_bar):\n        plt.text(i, v + 0.01, f\"{v:.2f}\", ha='center', fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    # Boxplot for intra-class distance distributions.\n    plt.figure(figsize=(8,6))\n    data = [intra_class_dists[label] for label in unique_labels]\n    plt.boxplot(data, labels=unique_labels)\n    plt.ylabel(\"Distance to Centroid\")\n    plt.title(\"Intra-Class Distance Distributions\")\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T07:05:09.530936Z","iopub.execute_input":"2025-03-27T07:05:09.531326Z","iopub.status.idle":"2025-03-27T07:05:09.565779Z","shell.execute_reply.started":"2025-03-27T07:05:09.531293Z","shell.execute_reply":"2025-03-27T07:05:09.564818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================\n# 5. Main Function\n# ====================\n\ndef main():\n    # Set your dataset root directory (update as needed)\n    root_dir = '/kaggle/input/qmsat-dataset/ATS-data'\n    sample_rate = 16000\n    batch_size = 128\n    contrastive_epochs = 30\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Prepare dataset and augmentations.\n    augmentations = AudioAugmentations(sample_rate=sample_rate, crop_size=sample_rate)\n    dataset = QMSAT(root_dir=root_dir, sample_rate=sample_rate, transform=augmentations)  # Using renamed QMSAT dataset\n    \n    # Split dataset into training and validation sets (80/20 split).\n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n    print(f\"Training samples: {len(train_dataset)} | Validation samples: {len(val_dataset)}\")\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n    \n    # Initialize the encoder.\n    encoder_model = QSMATATSEncoder(projection_dim=128).to(device)  # Using renamed encoder model\n    print(\"Model Architecture Summary:\")\n    print(encoder_model)\n    \n    optimizer_encoder = optim.Adam(encoder_model.parameters(), lr=1e-3, weight_decay=1e-4)\n    \n    # Train the encoder with self-supervised learning.\n    print(\"Starting Self-Supervised Training on\", device)\n    (train_losses, train_cosine, val_losses, val_cosine,\n     epoch_times, total_training_time) = train_self_supervised(encoder_model, train_loader, val_loader, optimizer_encoder, device, epochs=contrastive_epochs)\n    \n    # Plot training curves.\n    plot_loss_curves(train_losses, val_losses)\n    plot_cosine_curves(train_cosine, val_cosine)\n    \n    # Save training metrics to an Excel file.\n    excel_filename = \"/kaggle/working/self_supervised_training_metrics.xlsx\"\n    df_metrics = pd.DataFrame({\n        'Epoch': list(range(1, contrastive_epochs+1)),\n        'Train_Loss': train_losses,\n        'Train_Cosine': train_cosine,\n        'Val_Loss': val_losses,\n        'Val_Cosine': val_cosine,\n        'Epoch_Time_sec': epoch_times\n    })\n    writer = pd.ExcelWriter(excel_filename, engine='xlsxwriter')\n    df_metrics.to_excel(writer, sheet_name='Metrics', index=False)\n    writer.close()\n    print(f\"Training metrics saved to {excel_filename}\")\n    \n    # Save the trained encoder model.\n    model_save_path = \"/kaggle/working/trained_qsmatats_encoder.pth\"  # Saving with new model name\n    torch.save(encoder_model.state_dict(), model_save_path)\n    print(f\"Trained encoder model saved to {model_save_path}\")\n    print(f\"Total Training Time: {total_training_time/60:.2f} minutes\")\n    \nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T07:19:42.443127Z","iopub.execute_input":"2025-03-27T07:19:42.443565Z","iopub.status.idle":"2025-03-27T07:25:53.328965Z","shell.execute_reply.started":"2025-03-27T07:19:42.443531Z","shell.execute_reply":"2025-03-27T07:25:53.327820Z"}},"outputs":[],"execution_count":null}]}